# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UixzqoqH4s4GZBk85igRvGhKpOzgxDs_
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from tqdm.auto import tqdm

# install datasets
!pip install datasets
from datasets import load_dataset, load_metric, Dataset

# install transformers
!pip install transformers==4.28.0
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from transformers import DefaultDataCollator

dataset = load_dataset("persiannlp/parsinlu_entailment")
dataset = dataset.filter(lambda example: example['label'] in ["c", "e", "n"])
dataset

"""# BERT"""

model_checkpoint = "HooshvareLab/bert-base-parsbert-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

metric = load_metric("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

str_to_int = {"e": 0, "n": 1, "c": 2}
def tokenize_function(examples):
    tokenized_batch = tokenizer(examples["sent1"], examples["sent2"], truncation=True)
    tokenized_batch["label"] = [str_to_int[label] for label in examples["label"]]
    return tokenized_batch

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets

training_args = TrainingArguments(
    output_dir="parsbert",
    evaluation_strategy="epoch",
    logging_steps = 20,
    learning_rate=5e-5,
    num_train_epochs=3,
    warmup_steps=50,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy = "epoch",
    load_best_model_at_end=True,
    save_total_limit = 1,
    metric_for_best_model="accuracy",
    group_by_length = True,
    seed=0
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
trainer.train()

trainer.evaluate(tokenized_datasets["test"])

"""# XLM-RoBERTa"""

model_checkpoint = "xlm-roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

metric = load_metric("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

str_to_int = {"e": 0, "n": 1, "c": 2}
def tokenize_function(examples):
    tokenized_batch = tokenizer(examples["sent1"], examples["sent2"], truncation=True, max_length=128)
    tokenized_batch["label"] = [str_to_int[label] for label in examples["label"]]
    return tokenized_batch

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets

training_args = TrainingArguments(
    output_dir="xlm-r",
    evaluation_strategy="epoch",
    save_strategy = "epoch",
    logging_steps = 20,
    learning_rate=2e-5,
    num_train_epochs=8,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=100,
    load_best_model_at_end=True,
    weight_decay=0.001,
    save_total_limit = 1,
    metric_for_best_model="accuracy",
    group_by_length = True,
    seed=0
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
trainer.train()

trainer.evaluate(tokenized_datasets["test"])