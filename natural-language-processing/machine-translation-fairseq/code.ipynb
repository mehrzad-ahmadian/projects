{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq bitarray fastBPE hydra-core omegaconf regex requests sacremoses subword_nmt sacrebleu==1.5.1\n",
        "!pip install transformers==4.28.0"
      ],
      "metadata": {
        "id": "bccCywrV4AXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fef2fb4-437a-4e5f-b868-182cb3b2558e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastBPE\n",
            "  Downloading fastBPE-0.1.0.tar.gz (35 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting subword_nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting sacrebleu==1.5.1\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker==2.0.0 (from sacrebleu==1.5.1)\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.5)\n",
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Collecting mock (from subword_nmt)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, fastBPE, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291605 sha256=3585c724028ce004a74c89013d1c3862001c559e5991b032a936b4beec5fd10d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp310-cp310-linux_x86_64.whl size=806514 sha256=6223cae57a8a7f07745f8fe17811843588b7050ccebf277313f548aaa6c158c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/5d/b9/4b8897941ebc9e8c6cc3f3ffd3ea5115731754269205098754\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=c3e7594eeb07ddfd9e4b559a922eb3ee1c965bfdb1fddc69317ce20f8d6b024f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq fastBPE antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, fastBPE, bitarray, antlr4-python3-runtime, sacremoses, sacrebleu, omegaconf, mock, subword_nmt, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.3 fairseq-0.12.2 fastBPE-0.1.0 hydra-core-1.0.7 mock-5.1.0 omegaconf-2.0.6 portalocker-2.0.0 sacrebleu-1.5.1 sacremoses-0.1.1 subword_nmt-0.3.8\n",
            "Collecting transformers==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0)\n",
            "  Downloading huggingface_hub-0.19.3-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2023.7.22)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.19.3 tokenizers-0.13.3 transformers-4.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1bhLVc723C9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8900c0-0892-42cf-af8c-f8f81ae20f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  preprocessed_data.zip\n",
            "   creating: preprocessed_data/\n",
            "  inflating: __MACOSX/._preprocessed_data  \n",
            "  inflating: preprocessed_data/train.fa  \n",
            "  inflating: __MACOSX/preprocessed_data/._train.fa  \n",
            "  inflating: preprocessed_data/train.en  \n",
            "  inflating: __MACOSX/preprocessed_data/._train.en  \n",
            "  inflating: preprocessed_data/test.en  \n",
            "  inflating: __MACOSX/preprocessed_data/._test.en  \n",
            "  inflating: preprocessed_data/test.fa  \n",
            "  inflating: __MACOSX/preprocessed_data/._test.fa  \n",
            "  inflating: preprocessed_data/valid.en  \n",
            "  inflating: __MACOSX/preprocessed_data/._valid.en  \n",
            "  inflating: preprocessed_data/valid.fa  \n",
            "  inflating: __MACOSX/preprocessed_data/._valid.fa  \n"
          ]
        }
      ],
      "source": [
        "! unzip preprocessed_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_bash(shell_string):\n",
        "    with open('script.sh', 'w') as file:\n",
        "        file.write(shell_string)\n",
        "    !chmod 755 ./script.sh\n",
        "    !./script.sh"
      ],
      "metadata": {
        "id": "OK_NeCB44zxE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization + BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\"], continuing_subword_prefix=\"@\")\n",
        "for lang in [\"en\", \"fa\"]:\n",
        "    tokenizer.train(files=[f\"preprocessed_data/train.{lang}\", f\"preprocessed_data/valid.{lang}\", f\"preprocessed_data/test.{lang}\"], trainer=trainer)\n",
        "    text = \"test text for BPE\" if lang == \"en\" else \"متن تست بی پی ای\"\n",
        "    output = tokenizer.encode(text)\n",
        "    print(tokenizer.decode(output.ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLJ4XJ_lCult",
        "outputId": "3428bf49-751c-4053-efd4-b182e51b1668"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test text for\n",
            "متن تست بی پی ای\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fairseq_preprocess = \"\"\"\n",
        "rm -r data-bin/\n",
        "TEXT=/content/preprocessed_data\n",
        "fairseq-preprocess --source-lang en --target-lang fa \\\n",
        "    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
        "    --destdir data-bin/data.tokenized.en-fa \\\n",
        "    --workers 20 \\\n",
        "    --bpe bert \\\n",
        "    --log-format json \\\n",
        "\"\"\"\n",
        "run_bash(fairseq_preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V49POZAB1wpr",
        "outputId": "5f94360f-470f-4a28-ba43-ca3bbfb5004d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data-bin/': No such file or directory\n",
            "2023-11-16 12:12:42.577296: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 12:12:42.577361: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 12:12:42.577406: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 12:12:42.585941: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 12:12:43.548071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 12:12:46 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-11-16 12:12:46 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format='json', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe='bert', optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='en', target_lang='fa', trainpref='/content/preprocessed_data/train', validpref='/content/preprocessed_data/valid', testpref='/content/preprocessed_data/test', align_suffix=None, destdir='data-bin/data.tokenized.en-fa', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
            "2023-11-16 12:12:51 | INFO | fairseq_cli.preprocess | [en] Dictionary: 33040 types\n",
            "2023-11-16 12:12:55 | INFO | fairseq_cli.preprocess | [en] /content/preprocessed_data/train.en: 30000 sents, 544389 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-16 12:12:55 | INFO | fairseq_cli.preprocess | [en] Dictionary: 33040 types\n",
            "2023-11-16 12:12:56 | INFO | fairseq_cli.preprocess | [en] /content/preprocessed_data/valid.en: 400 sents, 6945 tokens, 3.01% replaced (by <unk>)\n",
            "2023-11-16 12:12:56 | INFO | fairseq_cli.preprocess | [en] Dictionary: 33040 types\n",
            "2023-11-16 12:12:57 | INFO | fairseq_cli.preprocess | [en] /content/preprocessed_data/test.en: 427 sents, 11267 tokens, 3.71% replaced (by <unk>)\n",
            "2023-11-16 12:12:57 | INFO | fairseq_cli.preprocess | [fa] Dictionary: 44136 types\n",
            "2023-11-16 12:13:03 | INFO | fairseq_cli.preprocess | [fa] /content/preprocessed_data/train.fa: 30000 sents, 520509 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-16 12:13:03 | INFO | fairseq_cli.preprocess | [fa] Dictionary: 44136 types\n",
            "2023-11-16 12:13:05 | INFO | fairseq_cli.preprocess | [fa] /content/preprocessed_data/valid.fa: 400 sents, 6500 tokens, 5.42% replaced (by <unk>)\n",
            "2023-11-16 12:13:05 | INFO | fairseq_cli.preprocess | [fa] Dictionary: 44136 types\n",
            "2023-11-16 12:13:07 | INFO | fairseq_cli.preprocess | [fa] /content/preprocessed_data/test.fa: 427 sents, 11376 tokens, 6.14% replaced (by <unk>)\n",
            "2023-11-16 12:13:07 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/data.tokenized.en-fa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fairseq_train = \"\"\"\n",
        "fairseq-train \\\n",
        "    data-bin/data.tokenized.en-fa \\\n",
        "    --arch transformer --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --max-epoch 50 \\\n",
        "    --patience 10 \\\n",
        "    --save-dir checkpoints \\\n",
        "    # --bpe bert \\\n",
        "    # --fp16 \\\n",
        "    # --reset-optimizer \\\n",
        "    --batch-size 64\n",
        "\"\"\"\n",
        "run_bash(fairseq_train)"
      ],
      "metadata": {
        "id": "5c7RF3o15olr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "MAX_EPOCHS = 2\n",
        "for i in tqdm(range(1, MAX_EPOCHS)):\n",
        "    ! fairseq-generate data-bin/data.tokenized.en-fa --path checkpoints/checkpoint{i}.pt --batch-size 128 --beam 5 --remove-bpe --log-format json --tensorboard-logdir 123"
      ],
      "metadata": {
        "id": "I34IWACd9pNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! fairseq-generate data-bin/data.tokenized.en-fa --path checkpoints/checkpoint_best.pt --batch-size 128 --beam 5 --remove-bpe --eval-bleu --results-path generate_results"
      ],
      "metadata": {
        "id": "FZf67OK3LjC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! fairseq-generate data-bin/data.tokenized.en-fa --path checkpoints/checkpoint_best.pt --batch-size 128 --beam 5 --remove-bpe --eval-bleu"
      ],
      "metadata": {
        "id": "ZR8zNFSOcVu7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}